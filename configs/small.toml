batch_size = 32
stack_size = 1
num_epochs = 100
precision = "f32"
debug_mode = false
do_profile = false
display = "progress"
# checkpoint_params = "precomputed/enb-120.ckpt"

[data]
dataset_name = "mptrj"
raw_data_folder = "data"
data_folder = "precomputed"
shuffle_seed = 29205
train_split = 30
test_split = 3
valid_split = 3
k = 32
batches_per_group = 0

[cli]
verbosity = "info"
show_progress = true

[log]
log_dir = "logs"
logs_per_epoch = 32
epochs_per_ckpt = 3
epochs_per_valid = 0.5
tags = []

[model]
outs_per_node = 128

[train]
max_grad_norm = 1.0
ema_gamma = 0.995
steps_between_ema = 32

[train.lr_schedule]
kind = "polynomial"
warmup_frac = 0.1
start_lr = 0.01
end_lr = 0.01
power = 1.0

[train.optimizer]
base_lr = 1.0
kind = "prodigy"
weight_decay = 1e-4
beta_1 = 0.9
beta_2 = 0.999

# [train.optimizer]
# base_lr = 1e-3
# kind = "adamw"
# weight_decay = 1e-4
# beta_1 = 0.9
# beta_2 = 0.999
# schedule_free = true

[train.loss]
energy_weight = 1.0
force_weight = 0
stress_weight = 0

[model.head]
inner_dims = []
activation = "silu"
final_activation = "Identity"
dropout = 0.1
residual = false
num_heads = 1
use_bias = false
normalization = "layer"

[model.node_embed]
embed_dim = 128
kind = "linear"

[model.rescale]
kind = "species-wise"
scale_trainable = true
shift_trainable = true
global_scale_trainable = true
global_shift_trainable = true

[model.hidden_irreps]
kind = "derived"
dim = 512
max_degree = 2
gamma = 1
num_layers = 4
min_gcd = 2

[train.loss.reg_loss]
loss_delta = 0
use_rmse = false

[model.edge_embed]
r_max = 5
r_max_trainable = false

[model.edge_embed.radial_basis]
num_basis = 12
kind = "bessel"
freq_trainable = false
use_sinc = true

[model.edge_embed.envelope]
kind = "exp"
cutoff_start = 0.8
c = 0.1

[model.interaction]
kind = 'simple'
residual = true
linear_intro = false
linear_outro = true

[model.interaction.message]
kind = "sevennet-conv"
max_ell = 2

# [model.self_connection]
# kind = 'linear'

[model.self_connection]
kind = "s2-mlp-mixer"

[model.self_connection.s2_grid]
activation = "silu"
res_beta = 18
res_alpha = 17
normalization = "integral"
quadrature = "soft"
use_fft = false

[model.self_connection.mlp]
inner_dims = [16]
activation = "silu"
final_activation = "silu"
dropout = 0.25
residual = false
num_heads = 2
use_bias = false
normalization = "layer"

[model.interaction.message.radial_weight]
inner_dims = []
activation = "silu"
final_activation = "Identity"
dropout = 0.0
residual = false
num_heads = 1
use_bias = false
normalization = "layer"
